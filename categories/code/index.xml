<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Code on Godot's Blog - 个人生活分享</title><link>https://iamgodot.com/categories/code/</link><description>Recent content in Code on Godot's Blog - 个人生活分享</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 15 Sep 2022 18:02:18 +0800</lastBuildDate><atom:link href="https://iamgodot.com/categories/code/index.xml" rel="self" type="application/rss+xml"/><item><title>Python 中的异常</title><link>https://iamgodot.com/posts/exception-in-python/</link><pubDate>Thu, 15 Sep 2022 18:02:18 +0800</pubDate><guid>https://iamgodot.com/posts/exception-in-python/</guid><description>在 Python 中，EAFP 的风格很受青睐，这种写法能让代码更加简洁，还可以避免一些重复判断和多线程竞争的问题。为此，了解并熟练使用异常是很重要的。
异常类 首先来看一下 Python 内置的异常类（有省略）：
BaseException +-- SystemExit +-- KeyboardInterrupt +-- GeneratorExit +-- Exception +-- StopIteration +-- StopAsyncIteration +-- ArithmeticError +-- AssertionError +-- AttributeError +-- BufferError +-- EOFError +-- ImportError +-- LookupError +-- MemoryError +-- NameError +-- OSError +-- ReferenceError +-- RuntimeError +-- SyntaxError +-- SystemError +-- TypeError +-- ValueError +-- Warning BaseException 是所有异常的老祖宗，但很少会用到，通常我们只需要 Exception，比如自定义一个错误类型：
# 命名习惯一般以 Error 结尾 class CustomError(Exception): def __init__(self, message, status): # 这里最好把参数都放进去，之后会统一存在 e.</description></item><item><title>Python Import 源码阅读</title><link>https://iamgodot.com/posts/source-code-of-python-import/</link><pubDate>Sat, 10 Sep 2022 18:20:40 +0800</pubDate><guid>https://iamgodot.com/posts/source-code-of-python-import/</guid><description>浅析一下 Python 中的 Import 机制。代码版本：3.11.0 - 4dd8219。
Module &amp;amp; Package Import 的对象就是各种各样的模块，即 Module。如何定义呢？官方文档这样描述：
A module is a file containing Python definitions and statements.
实际上，Module 可以是 Builtin，也可以是 C extension，但最常见的存在形式还是 *.py 文件。
一个 Module 同时也可能是 Package，此时它从文件升级为目录，就可以拥有 Submodule 了。Package 分为两种：
Regular package：这类 Package 必须包含一个 __init__.py 文件，代表了这个 Module(Package) 本身。 Namespace package：如果不存在 __init__.py，Python 会将其创建为此类 Package。Namespace package 的特殊之处在于同名的 Package 可以出现在多个目录下，而 Import 完成之后又可以统一使用。 不管是哪一种 Package，都会有 __path__ 属性，指向目录的路径。属性值是个列表，这对于 Namespace package 尤为重要。除此之外，一个 Module 还有：
__name__：模块名称。 __file__：文件位置。 __package__：主要是为了在 Relative import 时计算起点位置。 如果是 Package 则设置为 __name__。 如果非 Package 则设置为 Parent package 的名称（Top-level 的 Module 应为空字符串）。 如果以脚本执行，那么取值为 None。 How to Import 一般情况下 Import 都是通过 import 关键字完成的，可分为两大类：</description></item><item><title>Sum of Total Strength of Wizards</title><link>https://iamgodot.com/posts/sum-of-total-strength-of-wizards/</link><pubDate>Tue, 26 Jul 2022 10:41:55 +0800</pubDate><guid>https://iamgodot.com/posts/sum-of-total-strength-of-wizards/</guid><description>前两天做了一道算法题，虽然没能成功解决，但是是一道很有意思的题目。
抛开题面的包装不谈，核心内容就是给定一个数组，计算它的所有子数组的最小值与加和的乘积的总和。
（这里要注意子数组的定义，一定是连续的，如果不连续的话叫做子序列。）
比如对于 [1, 2, 3] 来说，一共有六种情况：
[1]: 1 * 1 = 1 [2]: 2 * 2 = 4 [3]: 3 * 3 = 9 [1, 2]: 1 * (1 + 2) = 3 [2, 3]: 2 * (2 + 3) = 10 [1, 2, 3]: 1 * (1 + 2 + 3) = 6 最后答案为 1 + 4 + 9 + 3 + 10 + 6 = 33。</description></item><item><title>关于 CORS</title><link>https://iamgodot.com/posts/about-cors/</link><pubDate>Tue, 17 May 2022 23:42:11 +0800</pubDate><guid>https://iamgodot.com/posts/about-cors/</guid><description>说起 CORS，就不得不先提到 SOP(Same-origin policy)：浏览器打开的网页只可以对该网页的同源网站发起请求。注意，受约束的主要是脚本代码，不包括图片或者 CSS 等资源（字体文件是个例外）。同源的定义包括三部分，即协议、域名和端口都要保持一致。
为了缓解 SOP 带来的严格限制，有几种主流的解决方案可以选择：
CORS JSONP：利用 &amp;lt;script&amp;gt; 标签来请求非同源地址的 JSON 响应，同时配合一个预先定义的回调函数来处理响应数据。 WebSocket：WS 连接并不受同源策略的约束，但是在建立连接时服务端也需要判断 headers 中的 Origin 是否可以接受。 其中 CORS 应该是最实用的一种，相比 JSONP 只支持 GET 请求，前者扩展了各种 HTTP 方法的跨域调用。
CORS(Cross-origin resource sharing)，是一种跨域共享资源的机制，它利用特定的 Headers 来保证跨域请求的安全性，这些请求分为两类：简单请求和非简单请求。
简单请求，包括 GET、HEAD 和 POST，这里 POST 的 Content-Type 仅限于下面三种：
application/x-www-form-urlencoded
multipart/form-data
text/plain
对于这些请求来说，只需要保证 Access-Control-Allow-Origin 中匹配了当前网页的域名即可，如果是 * 的话表明所有的域名都是允许的。
非简单请求，比如 Content-Type 为 application/json 的 POST，会增加一次额外的 Preflight 请求，即先发送 OPTIONS 请求给服务器，然后通过响应中的一系列 Headers 决定是否可以进行真正的请求。这些 Headers 包括：
Access-Control-Allow-Methods：服务器允许的跨域方法，比如 POST。 Access-Control-Allow-Headers：服务器允许的跨域头部，比如 Content-Type。 Access-Control-Max-Age：Preflight 请求结果的缓存时间，默认为 5s。 另外，如果想在 Chrome 中查看 Preflight 请求的话，打开 Network 标签，点击 Other filter 就可以看到了。</description></item><item><title>Bottle 框架源码阅读</title><link>https://iamgodot.com/posts/source-code-of-bottle/</link><pubDate>Fri, 22 Apr 2022 21:53:29 +0800</pubDate><guid>https://iamgodot.com/posts/source-code-of-bottle/</guid><description>写这篇文章最开心的一点是终于可以用这张截图了：
相比名声在外的 Django/Flask/FastAPI，Bottle 可以说是非常不起眼了，甚至很多人并不知道它的存在。其实在很多方面，这个框架都极其优秀：
速度：截止到 2022-04-13，Bottle 在一众 Python Web 框架的测评中名列第二，要知道这可是十年以上的老前辈了。 易用性：Bottle 早在 Flask 之前就使用了装饰器来定义路由，此外还有全局可用的 Request/Response 对象。 文档：不仅将框架本身的使用讲得很清楚，还总结了很多 Web 场景下的解决方案。 代码质量：虽然为了 Python 2 做了不少兼容，但是代码很精炼，而且 Pythonic。 其他：Bottle 坚持单模块以及无第三方库依赖；仓库仍然在积极维护中。 换作几年前，我会一开始就使用并将 Bottle 研究透彻，而不是让自己淹没在 Django 浩瀚如烟的文档中。下面开始梳理 Bottle 源码的阅读理解。因为代码量不大，所以就直接看最新的版本了：0.11.1 - 5a6c620。
Web 框架的基本元素 参考 The Hitchhiker&amp;rsquo;s Guide to Python 的说法，一个 Web 框架要满足的基本功能：
URL Routing Request and Response Objects Template Engine Development Web Server 从后端的角度来讲更重要的是 1、2、4 三项，其中 1 负责转发请求到对应的视图函数，2 是对 HTTP 协议元素的解析处理，而 4 决定了服务的部署方式和基础性能。
Bottle 在这几方面都做了很好的实现：路由上提供了通配符匹配和装饰器接口；请求和响应对象作为全局对象存在并保证了线程安全；Server 部署除了 Python 自带的 wsgiref 还支持绝大多数的 WSGI Server。</description></item><item><title>Python 中的 TLS 是如何实现的</title><link>https://iamgodot.com/posts/sourcecode-of-python-threadlocal/</link><pubDate>Mon, 11 Apr 2022 16:13:49 +0800</pubDate><guid>https://iamgodot.com/posts/sourcecode-of-python-threadlocal/</guid><description>TLS(Thread Local Storage)，或者说 Threadlocal，可以说是一种并发编程的常用模式，既实现了线程之间的资源隔离，又满足了全局变量的使用。
从 TLS 出发，这篇文章研究了 Python 中的 Threadlocal 是如何实现的，比如自带的 threading.local，再比如 Flask 框架中 Local 对象。
Why Threadlocal 先思考一下为什么要用 Threadlocal，这就不得不提到线程安全。Race condition 说到底是因为数据共享和非原子操作，这可以体现在函数的两种基本写法：一种是显式地传参（参数对象也可能变化？这也是为什么最好不要传递可变对象），没有共享自然安全；另一种就是全局对象，这么写既简化了函数签名，代码也比较清晰，缺点就是很容易出现线程不安全的问题，所以经常会和锁配合使用。
而 Threadlocal 就结合了两者的优点，在共享全局变量的同时，保证每个线程操作的都是自己独有的数据对象。
对比一下 Django 和 Flask 两大框架就会发现，前者总是在视图函数中显式声明 request 参数，而后者的只需要 import 一次就可以到处使用。在 Flask 的文档中，Armin Ronacher 也提及了这一点：
For example, Flask uses thread-local objects internally so that you don’t have to pass objects around from function to function within a request in order to stay threadsafe.
不过 Flask 并没有直接使用 Python 内置的 threading.</description></item><item><title>Python Logging 源码分析</title><link>https://iamgodot.com/posts/sourcecode-of-python-logging/</link><pubDate>Sat, 09 Apr 2022 09:52:51 +0800</pubDate><guid>https://iamgodot.com/posts/sourcecode-of-python-logging/</guid><description>阅读了源码之后，我对 Python Logging 模块的几大疑惑都得到了解答：
为什么 Logger 和 Handler 都有 setLevel 方法？
Logging 中会出现 Race condition 吗？（感觉都是很直接的 write 操作）
正式环境中想看日志又没办法动态调整 logLevel，感觉很鸡肋。
用起来好像还不如 print 方便。
会有性能问题吗？
日常使用 首先要了解下 Logging 的用法。
1. 配置 基本上有三种方式，代码、文件和字典。先看下如何用代码设置：
import logging # create logger logger = logging.getLogger(&amp;#39;simple_example&amp;#39;) logger.setLevel(logging.DEBUG) # create console handler and set level to debug ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # create formatter formatter = logging.Formatter(&amp;#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s&amp;#39;) # add formatter to ch ch.</description></item><item><title>关于 Pager</title><link>https://iamgodot.com/posts/about-pager/</link><pubDate>Sat, 02 Apr 2022 14:09:53 +0800</pubDate><guid>https://iamgodot.com/posts/about-pager/</guid><description>好久没有更新了，最近研究了下如何用 Python 实现 Pager 的功能，这里指的是 Terminal 中的 Paging 程序，比如 less。
Why Pager Pager 在大段文字的展示中很常见，比如 Linux 的 man page，而 $PAGER 就是用来指定默认 Paging 程序的环境变量。Python shell 里面的 help() 会默认翻页显示，IPython 的 ? 则更胜一筹，能够判断当前屏幕的可用空间来决定是否 Paging。
Less 应该算是最流行的 Pager 了，相比于 more，它同时支持前进和后退翻页，而且因为不需要一次性读取整个文件，它的启动速度在打开大型文件时要远远快于 vi。因此，许多 Pager 都是通过启动系统自带的 less 程序来实现。
Don&amp;rsquo;t Reinvent the Wheel 轮子总是有的，而且还很多，这里说几个比较好用的：
Pydoc 的 pager Click 的 echo_via_pager IPython 的 page Pydoc 是 Python 自带的，已经稳定存在了很多年，轻巧好用；Click 的实现类似，而且支持传入一个 generator；IPython 的 page 更加强大，可以自动判断当前的屏幕大小，再结合一个 screen_lines 参数来计算最终的可用空间。
再说说这几个轮子的实现，基本思路都是上面提到的调用系统 Pager。因为要兼容五花八门的操作系统，大致上又分为三种处理方式：
理想情况下是使用 PIPE。因为打开的系统 Pager 必然是子进程，而 PIPE 通过内存中的缓冲区实现了 IPC，这样既不用一次性读取所有数据，后续的 write 操作效率也高。</description></item><item><title>Python 中的闭包</title><link>https://iamgodot.com/posts/closure-in-python/</link><pubDate>Sat, 29 Jan 2022 17:55:18 +0800</pubDate><guid>https://iamgodot.com/posts/closure-in-python/</guid><description>def f1(): l = [] def c(): l.append(1) def f2(): a = 1 def c(): print(a) a = 2 类似 f1 和 f2 中的闭包写法，之前总是在用了前者多次之后，再写后一种就报错，感觉很莫名其妙，明明都差不多。研究了下发现，其实这是 Python 闭包的 BugFeature，理解之后反而觉得这样的设计是合理的。
首先说 Closure，也就是闭包，特指内部的函数及其引用的超出本身作用域的对象，总是在函数嵌套时发生。在 f1 中，c 就是一个闭包函数，同时 l 也算作其中的一部分。因为 c 使用了 l 导致延伸了原有的作用域，所以才有闭包的产生。
再看 f2，如果我们只对 a 做 read 操作是不会引发问题的，由于 c 中尝试了赋值操作，才导致了 UnboundLocalError. 这是因为 Python 解释器会假定函数中赋值的变量是局部变量，而 c 中本身并没有定义 local 的 a 变量；其次，异常在 print(a) 时就会抛出，不会等到 a = 2 的执行。
那为什么 f1 没问题呢，是因为列表为可变对象，append 操作只是更新了里面的内容，并不存在赋值。
def f(): l = [] def c(): l.</description></item><item><title>关于 HTTP Auth</title><link>https://iamgodot.com/posts/about-http-auth/</link><pubDate>Fri, 28 Jan 2022 10:16:19 +0800</pubDate><guid>https://iamgodot.com/posts/about-http-auth/</guid><description>Auth 代表了 Authentication 和 Authorization 两个概念，也就是认证与授权。基于 HTTP，两者得以遵循一定的标准，SSL/TLS 之后，又出现了 OAuth 2.0，让授权也简单了许多。
Authentication 认证相对来说比较直接，核心就是对 Credential(e.g. username/password) 的验证。HTTP 提供了多种认证方案，比如最常见的 Basic auth, Digest access 和 Bearer.
Basic auth 具体来说就是服务器用 WWW-Authenticate 表示需要认证，比如 WWW-Authenticate: Basic realm='Accessing to xx site'，客户端则通过 Authorization 提供相关信息：Authorization: Basic Zm9vOmJhcg==，后面的一串编码是对用户名密码明文进行 base64 的结果，即可以直接从中 decode 出原始信息 foo:bar. 没有 HTTPS 的保护，这样很不安全，所以 Apache/Nginx 对 BA 的实现都会使用密码的哈希结果而不是原文，拿后者举例：
http { server { location / { auth_basic &amp;#34;Accessing to xx site&amp;#34;; auth_basic_user_file /path/to/authfile; } } } 然后需要在 authfile 中保存 username/password pair，比如 sudo htpasswd -c /path/to/authfile user1，htpasswd 是 Apache 提供的专门用来生成 BA 使用的 Credential file 的工具。不用额外安装，我们直接用 openssl 代替：</description></item><item><title>关于 SSH 端口转发</title><link>https://iamgodot.com/posts/ssh-port-forwarding/</link><pubDate>Wed, 22 Dec 2021 11:36:49 +0800</pubDate><guid>https://iamgodot.com/posts/ssh-port-forwarding/</guid><description>SSH 的端口转发很实用，但我总觉得难以理解和记忆，直到最近才有所好转。
因为又派上用场了。以前基本只做做内网穿透，现在更多地拿来绕过防火墙。自己的服务器，大多数端口虽然都是被禁用的（至少禁止入网，这也是正常的安全措施），但是想要连接的话直接本地端口转发就可以了。
TL;DR 本地端口转发在当前机器上设置，然后从本机出发，通过另一台机器，连接其他的机器。适用于防火墙的绕过、多重 SSH 登录等。
远程端口转发在当前机器上设置，然后从另外一台机器出发，通过当前机器，连接本机或者其他的机器。适用于 NAT 网络穿透、暴露内部网络服务等。
本质上都是先建立 SSH 会话，形成隧道，然后在上面进行正向或反向的数据传输。
Local Port Forwarding 为什么叫做本地呢，我想有两个原因：
转发的端口在当前（执行 SSH 命令这台）机器上 请求是从当前机器发出的 当前机器就是我的笔记本，另外一台是服务器。比如，在服务器上部署一个应用，开放给 8000 端口，但是被墙掉了，没办法在本地调试，怎么办？防火墙肯定开放了 SSH 登录的端口，比如 22，那么就让请求从本地的端口发送到服务器的 22 端口，再转发到 8000 端口，最后原路返回。我可以设置本地的端口也是 8000，这样直接用 localhost:8000 来访问应用就好了。
转发的重点在于本地的 8000 端口和服务器的 22 端口之间，因为请求到了服务器之后可以给应用的 8000，也可以给其他的机器，只要服务器能连接到：
# ssh -L local_port:dest_addr:dest_port server # Local 8000 &amp;lt; -- &amp;gt; Server 22 &amp;lt; -- &amp;gt; Server 8000 # -fNT 让 ssh 不要打开服务器 shell，并且转为后台运行 # server 隐含了使用 22 端口登录，当然也可以在 ssh config 中设置任意登录端口 ssh -fNT -L 8000:localhost:8000 server 注意这里的 dest 对应的 src 是 server，也就是说 localhost 及后面的 8000 都是 server 的 IP 和端口。可以理解为 server 是中介，整条通路是 local -&amp;gt; server -&amp;gt; dest.</description></item><item><title>From BitTorrent to Firewall</title><link>https://iamgodot.com/posts/from-bittorrent-to-firewall/</link><pubDate>Mon, 20 Dec 2021 17:00:00 +0800</pubDate><guid>https://iamgodot.com/posts/from-bittorrent-to-firewall/</guid><description>服务器能做什么？在 Awesome-Selfhosted 里可以找到上百种答案。如果带宽不算太小的话，那么 BT 下载是个不错的尝试。借着 No Time to Die 的上映我开始重温 007 系列，从皇家赌场到幽灵党，在服务器上的下载体验是很好的。
BitTorrent 在此之前，我基本上把 BT、种子、磁力、迅雷下载当成同一种东西。下载电影？先找种子或者磁力链接，打开迅雷下载，然后视速度决定要不要开个会员。
实际上这完全曲解了 BT 下载。
首先 BitTorrent 是一种网络协议。还记得计算机网络一开始就提到过除了 C/S 架构之外，还有 P2P(Peer-to-peer)，也就是网络中的各个节点都扮演了同等的角色，既是客户端也是服务器。BT 基于 P2P 实现了去中心化的文件分享，让网络数据的传输不再仅限于服务器的能力，而是共享带宽，每个人下载的同时也在上传，所以越多人参与速度就越快。
类似于 HTTP 和 FTP，BT 也是基于 TCP/IP 的一种应用层协议。基本上它是这么运作的：
我有一部电影，想把资源分享到网络，要先提供一个种子文件
种子文件实际上就是个文本文件，里面主要记录两部分信息
Trackers: 就是 Tracker 服务器的地址，这个服务器不是用来下载资源的，而是用于获取其他 Peers 的联系方式 Files: 一个视频文件会被（逻辑）划分为很多个虚拟分块，每块的索引和验证码都包含在这里 接下来我把种子文件发布出去，等待别人下载
这时候有人获取到种子了，于是开启了 BT 客户端下载
客户端先解析种子文件中的信息，找到 Tracker，然后询问有哪些 Peers
因为是第一个下载者，所以 Tracker 告知 Peer 目前只有我，也就是发布者
之后对方会尝试与我互连，然后根据 Files 信息交换数据，这里基本就是我上传给对方
下载的过程会以分块为单位进行，每块完成下载后会根据验证码再做校验
如果这时又有一个人开始下载，那么我和这第一个下载者都会贡献上传
随着更多用户的参与，（上传）下载的速度就会越来越快
可以发现，整个过程中 Tracker 是很关键的一步，如果没有有效的 Tracker 提供 Peers，后面的下载都无法开始。所以如果你的 BT 下载没有速度，首先要尝试多添加一些 Tracker 服务器，比如 TrackersList.</description></item><item><title>Python heapq 源码阅读</title><link>https://iamgodot.com/posts/sourcecode-of-python-heapq/</link><pubDate>Mon, 29 Nov 2021 23:26:37 +0800</pubDate><guid>https://iamgodot.com/posts/sourcecode-of-python-heapq/</guid><description>Heap 作为一种重要的数据结构，有许多应用场景，比如优先级队列，每次出队的都是最大值或者最小值的元素。很多语言都集成了相关实现，比如 Java 的 PriorityQueue，而 Python 提供了 heapq 模块。
因为 Heap 通常用数组而不是链表存储，所以 Python 里面的 Heap 实质上就是一个列表，而 heapq 提供的几个函数也是以列表对象作为参数的：
from heapq import heappush, heappop, heappify, heapreplace, heappushpop heap = [] heappush(heap, 1) item = heap[0] # 第一个元素代表堆顶元素 heappop(heap) heapify([3, 2, 1, 5, 6, 4]) # 把普通列表转化为堆结构 [1, 2, 3, 4, 5, 6] heapreplace([3, 4, 5], 1) # 直接将堆顶元素 3 替换为 1，最后堆结构为 [1, 4, 5] heappushpop([3, 4, 5], 1) # 先将 1 插入堆中，再 pop 出堆顶元素，最后堆结构为 [3, 4, 5] 为什么 heapq 提供的是最小堆而不是更常见的最大堆呢？这就得从源码中找答案了。</description></item><item><title>Python OrderedDict 实现 LRU 缓存</title><link>https://iamgodot.com/posts/sourcecode-of-python-ordereddict/</link><pubDate>Sun, 28 Nov 2021 17:50:51 +0800</pubDate><guid>https://iamgodot.com/posts/sourcecode-of-python-ordereddict/</guid><description>LRUCache 是一种经典的缓存机制，它的基本思路是按照最近使用的时间对元素排序，在清理时优先把搁置最久的删除掉。
如果不想给每个缓存元素都记录一个时间戳的话，可以应用哈希链表来简单地实现 LRU 算法。也就是对一个哈希表中的所有元素增加指针，从而串起一个双链表，这样既可以快速 get value，又可以通过把最近使用过的元素放到头部来维护顺序，删除的时候从末尾开始就好了。
手写双链表并不困难，但是借助 OrderedDict 的话，可以写出非常简短的代码：
from collections import OrderedDict class LRUCache: def __init__(self, capacity): self.capacity = capacity self.hashtable = OrderedDict() def get(self, key: int) -&amp;gt; int: if key in self.hashtable: self.hashtable.move_to_end(key, last=False) return self.hashtable[key] return -1 def put(self, key: int, value: int) -&amp;gt; None: self.hashtable[key] = value self.hashtable.move_to_end(key, last=False) if len(self.hashtable) &amp;gt; self.capacity: self.hashtable.popitem() 其中最神奇的就是 move_to_end 和 popitem 方法（后者默认是弹出最后面的元素）的使用，这也得益于 OD 可以保证 key-value pair 的顺序。那么
OD 是如何做到的呢？其实还是双链表，下面是它的 Python 实现：</description></item><item><title>Numeric Strings in Python</title><link>https://iamgodot.com/posts/numeric-strings-in-python/</link><pubDate>Sun, 21 Nov 2021 17:44:36 +0800</pubDate><guid>https://iamgodot.com/posts/numeric-strings-in-python/</guid><description>Python 的字符串自带了三种判断字符是否为数字的方法，但实际用处却相差很多。
TL;DR 三种方法 isdecimal &amp;lt; isdigit &amp;lt; isnumeric，即包含的范围越来越大 除了 ASCII 字符以外，对于 Unicode 的字符也都覆盖在内 三种方法对于小数点和负号都会返回 False 三种方法对于空字符串都会返回 False 比较简便判断数字字符串的方法：直接使用 float 方法并检测 ValueError Decimal &amp;amp; Digit &amp;amp; Numeric 对于 isdecimal, isdigit 和 isnumeric 三种方法，目的并不是判断字符串是不是一个有效数字，而是针对每一个字符的校验：
isdecimal: 判断字符串中的字符是否都为 Decimal，也就是在 Unicode 中类别为 Nd 的字符 isdigit: 除了 isdecimal 包含的范围之外，还会判断字符是否都为 Digit，即 Unicode 的 Numeric_Type 为 Digit 或 Decimal isnumeric: 除了 isdigit 包含的范围之外，还会判断字符是否都为 Numeric，即 Numeric_Type 为 Numeric 所以这三种方法覆盖的字符范围，每一个都是前一个的超集。对于超出 ASCII 字符之外的效果，比如：
&amp;ldquo;０１２３４５６７８９&amp;rdquo; 这种 Full-width 字符串 isdecimal 会判定为 True，后两个方法也一样 &amp;ldquo;⓪①②③④⑤⑥⑦⑧⑨&amp;rdquo; 这种 Circled-digit 字符串 isdecimal 判定 False，但 isdigit 和 isnumeric 为 True &amp;ldquo;一二三四五六七八九十壹貳參肆伍陸柒捌玖拾&amp;rdquo; 这种中文数字字符串只有 isnumeric 才会判定为 True 总之这几种方法有更广泛的用途，根本不是为了简单的 ASCII 数字字符串的判断。即使用来做判断的话，局限性也非常大，因为如果包含小数点和负号，三个方法都会返回 False.</description></item><item><title>From Wireshark to Linux Capabilities</title><link>https://iamgodot.com/posts/from-wireshark-to-linux-capabilities/</link><pubDate>Sun, 21 Nov 2021 13:39:25 +0800</pubDate><guid>https://iamgodot.com/posts/from-wireshark-to-linux-capabilities/</guid><description>Tcpdump 和 Wireshark 是抓包必备的程序，但是由于需要截取网络数据包，所以在 Linux 下必须以 root 的身份来运行。每次都 sudo 执行不方便也并不安全（对 Wireshark 来说捉包只是一小部分功能），解决方案当然有，在寻找的过程中我了解到了 Capabilities 的冰山一角。
TL;DR 可以通过设置 Setuid 以 root 身份执行，但如此以来赋予了过高的权限（也没有必要）。 Linux 下用 Capabilities 把系统权限划分成多个条目，以此实现细粒度地提升程序的执行能力。 Setuid 先盗一张图复习下 Linux File Permission 的基础知识：
除了 rwx 之外还存在三种特殊类型，即是为了在更高权限下运行程序：
Setuid: 程序的运行者不再是执行者，而是变成文件的所有者 Setgid: 程序的运行群组变成了文件的所在群组，如果给目录设置，那么其中新建的文件所有群组会变成目录的群组而不是执行者的群组 Sticky bit: 针对目录设置，目录下的文件只有所有者和 root 能够重命名、移动和删除 在 Linux 中 sudo 是 Setuid 最好的例子，而 crontab 和 tmp/ 分别是 Setgid 和 Sticky bit 的典型应用。
在命令行中测试：
# Setuid ➜ ~ umask -S u=rwx,g=rx,o=rx ➜ ~ umask # 掩码是 022，所以默认文件的权限是 666-022=644，而目录则是 777-022=755 022 ➜ ~ touch foo.</description></item><item><title>Overflow in Python</title><link>https://iamgodot.com/posts/overflow-in-python/</link><pubDate>Fri, 12 Nov 2021 17:20:10 +0800</pubDate><guid>https://iamgodot.com/posts/overflow-in-python/</guid><description>犹记得在 Java 中，int 占用 4 bytes 所以大小限制在 -2^32 和 2^32 -1 之间，如果考虑到溢出的情况，就要用 long 类型来转换。
但是在 Python 中似乎从来没有考虑过类似的问题，也不记得 int 是占几个字节，那么是说 Python 的数字永远不会溢出吗？不可能吧。
答案是对于 int 类型来说，可以认为不会；而 float 类型则需要注意溢出的情况。
首先说 int 类型，打印 sys.maxsize 可以看到 Python 会根据系统是 32 位还是 64 位来确定一个上限值，这点与 Java 等语言一致。不同的是我们仍然可以使用比这个上限大（得多）的整数，因为 Python 支持的是 Arbitrary-precision arithmetic。也就是说为了突破 CPU 字长带来的运算限制，通过在软件层面模拟计算过程，是可以完成更高位数和精度的运算的。比如在公钥加密的场景下，经常需要对上百位的整数进行运算，这时候就需要在软件层面支持。
这确实说明我们可以使用任意长度的 int 数字，只要不超出内存限制的话。因为如果给 Python 解释器分配了 2GB 内存，但是 2 * 1024 * 1024 * 1024 * 8 这么多位也不够表达的话，还是会出错的，只是并非 Overflow，而是 MemoryError.
再说 float. 打印 sys.float_info.max 可以看到 float 的上限值，如果超出之后是会报错的，即 OverflowError.</description></item><item><title>Understand Recursion Better</title><link>https://iamgodot.com/posts/understand-recursion-better/</link><pubDate>Mon, 08 Nov 2021 18:18:10 +0800</pubDate><guid>https://iamgodot.com/posts/understand-recursion-better/</guid><description>在 Simple Recursion 之后，我一度把递归当作一种算法。但通过比较 Divide and Conquer 和 Dynamic Programming，我才发现之前的理解有点问题。
一切还是要从 Algorithmic Paradigm 说起：
An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program.
算法范式，是在算法的层面上抽象出来的一种更泛化的思想，常用的有：
Brute-force search 暴力解法 Backtracking 回溯算法 Greedy algorithm 贪心算法 Divide and conquer 分治法 Dynamic programming 动态规划 Divide and Conquer 的基本思路是把复杂的问题分解成多个类似的简单问题，解决之后再组合起来得到最终结果。这种算法有很多应用，比如排序中的 Merge Sort，先将数列分解成单个元素，然后再归并，这时子数组都已经排好顺序了，所以过程很快。</description></item><item><title>Record Terminal as GIF</title><link>https://iamgodot.com/posts/record-terminal/</link><pubDate>Sun, 07 Nov 2021 21:35:44 +0800</pubDate><guid>https://iamgodot.com/posts/record-terminal/</guid><description>这几天在做一个 CLI 项目，因为涉及到命令行操作，所以想录制一段 GIF 放在 README 中展示。
印象里这种工具都是 JS 写的，但搜了搜居然发现有个 Python 的实现：asciinema
用法很简单，就是安装好之后在命令行执行 asciinema rec 便会启动一个新的 Shell 并开始录制，录制的时候就像正常使用 Terminal 一样即可，完成之后按 Ctrl-D 或者 Exit 退出，asciinema 会把录制好的 cast 文件保存到本地，也可以选择上传到他们的网站：asciinema.org.
那么 cast 文件是什么，又怎样得到 GIF 呢？其实这是 asciinema 自己定义的一种文件格式：
A CAST file is a record of a terminal session recorded using asciinema, an open source terminal recording program. It contains a JSON-formatted header and a timestamped record of the text typed and printed during a terminal session.</description></item><item><title>Look and Say</title><link>https://iamgodot.com/posts/look-and-say/</link><pubDate>Sun, 07 Nov 2021 16:59:50 +0800</pubDate><guid>https://iamgodot.com/posts/look-and-say/</guid><description>无意中看到一种叫 look-and-say 的数列，很有意思，有点儿 Fibonacci 的感觉。数列如下：
1, 11, 21, 1211, 111221, 312211, 13112221, 1113213211, &amp;hellip;
从第二位开始，每个数字都是对前一个数的计数结果的描述。比如 11 表示前面的 1 有一个 1，而 21 表示前面的 11 有两个 1，1121 表示 21 有一个 2 和一个 1，依次类推，可以无穷循环出新的结果。（除了 22 这个数字，因为会一直重复 22 本身）
后来查了查才发现原来 Leetcode 也有这道题，不过名字叫做 count-and-say。基本就是给 n 然后求此数列的第 n 项结果。
想了想思路并不难，无非是对一个数字的所有位数循环再计数就好了，不过写的时候很犯难，竟然还写出了一个无比冗长的 for 循环加 if 面条代码。这让我意识到了自己对于循环的认识有多么不够深刻。
之所以犯难，其实是不知道怎样把几种逻辑合并在一起，如果粗暴地列举所有分支大概是这样：
def find_next(num: str): result = &amp;#39;&amp;#39; cur, count = num[0], 1 size = len(num) for i in range(1, size): if num[i] == cur and i == size - 1: # count 加一 # 更新 result elif num[i] == cur: # count 加一 elif i == size - 1: # 更新 result # 更新 cur&amp;amp;count # 更新 result else: # 更新 result # 更新 cur&amp;amp;count # 还有一种情况就是并没有进入 for 循环 # 那么也需要更新 result 直接按照这些分支把代码填满肯定很难看，所以要挑选合并。看起来只有第二个分支不需要更新 result，于是我决定把它单拎出来：如果 num[i]==cur 那么 count 加一；否则就更新 result 以及 cur&amp;amp;count.</description></item><item><title>Find Median</title><link>https://iamgodot.com/posts/find-median/</link><pubDate>Wed, 03 Nov 2021 17:07:04 +0800</pubDate><guid>https://iamgodot.com/posts/find-median/</guid><description>祸从天降的一天。
早上起不来，于是刷手机清醒一下，突然看到一个 ACMer 楼主提到自己没有刷过 Leetcode，面试的时候差点儿被打脸了。
看了一下题目，要求是 O(logn) 的复杂度，默默想了想，没有特别清晰的思路。
结果翻了翻评论，很多人都在蜻蜓点水般地表示二分查找不断分割就可以了。
要那么简单还用你说吗？起床了起床了。
题目是这样的：
给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的中位数。
我本来的想法是归并之后计算中位数，但只能做到 O(m+n)，再优化感觉只能二分了。
于是又开始想分别取两个数组的中位数，比较之后就可以各扔掉一半，然后对两个折半的数组继续取中位数比较。
比如一开始找到的中位数是这样：[&amp;hellip;5&amp;hellip;], [&amp;hellip;10&amp;hellip;]，那么 5 的左边和 10 的右边就可以丢掉了，因为最终的中位数肯定在 5 和 10 中间。
如果接下来是这样：[5..8&amp;hellip;], [&amp;hellip;7..10]，那么 8 的右边和 7 的左边也可以丢掉，因为比 8 大的元素数量达不到这两个小数组的一半，所以中位数不会在里面，比 7 小的同理。
一直重复这个流程，最后得到的肯定就是一个或两个数，平均下就好了，时间复杂度也是符合要求的 O(logn).
结果等到看完标答后我才反应过来自己错在哪里了，这是后话。（因为 m 和 n 不一定一样大，所以显然折半的逻辑不对）
最优的解决方案是可以做到 O(log min(m, n)) 的，核心也是二分法，不过思路要复杂一些：
首先明确中位数的定义。对于奇数个数字来说是最中间的元素，偶数则是中间两个元素的平均值。 要做的是将两个数组各分一刀，假设划分的下标分别是 i 和 j，那么 nums1 被分成 nums1[0, i], nums1[i:]，而 nums2 分为 nums2[0, j], nums2[j:]. 因为数组中元素的数量是奇是偶不一定，所以规定好是奇数的话多的一个放到左边。如此 i 的位置就是右边部分的第一个，而左半边正好有 i 个元素。 下标 i 和 j 之间是存在一个等式关系的，因为是中位数，所以 i + j 等于元素总数的一半，或者一半多一个（因为说好了左边多一个嘛）。那么就有 i+j=(m+n)/2 | i+j=(m+n)/2+1，合并起来写成 (m+n+1)/2.</description></item><item><title>(Re)Write recursions easily</title><link>https://iamgodot.com/posts/simple-recursion/</link><pubDate>Mon, 25 Oct 2021 08:56:54 +0800</pubDate><guid>https://iamgodot.com/posts/simple-recursion/</guid><description>近来刷题，觉得递归实在神奇。对于有的题目，用循环实现思路很清晰，改成递归却变得非常抽象；另一些相反，递归的做法既简洁又容易理解，但换成迭代就怎样都不明白了。经过几次痛苦的思考之后，我发现其中还是有迹可循的，按照套路来做，至少思路不会走得太偏。
在维基上面发现的关于递归的笑话：
To understand recursion, you must understand recursion.
Write Recursion 审题分析过后，如果选择用递归，需要先明确两个最重要的组成部分：
边界条件，保证函数能够返回，不会无限递归下去 递进关系，也就是上下层递归调用结果的等式规律 比如求 factorial：
def factorial(n): if n &amp;lt;= 2: return n return factorial(n - 1) * n 如果必要的话，还得考虑的是如何维护上下文的状态信息，目的是计算最终的返回值，有两种办法：
使用全局（outer scope）变量 利用参数进行传递 下面的写法就是利用了参数来保存中间的计算结果：
def factorial(n, res): if n &amp;lt;= 2: return res * n return factorial(n - 1, res * n) 也就是所谓的尾递归，对于可以优化尾递归的语言确实可以看成这种方式是从上至下的，而常规的递归则是从下到上。但我理解递归都是先下去再上来的，尾递归只是选择了一去不返而已。
上面的例子比较简单，反转链表的代码就比较难理解一点：
def reverse_list(head): if not head or not head.next: return head new_head = reverse_list_by_recursion(head.next) head.next.next = head head.</description></item><item><title>Use setxkbmap for Keyboard Mapping</title><link>https://iamgodot.com/posts/use-setxkbmap-for-keyboard-mapping/</link><pubDate>Sat, 16 Oct 2021 04:57:10 +0800</pubDate><guid>https://iamgodot.com/posts/use-setxkbmap-for-keyboard-mapping/</guid><description>这是一篇使用（过） Linux Xorg without any DE 以及笔记本外接键盘的人才能理解其中辛酸苦痛的抒情科普文。
TL;DR 外接键盘不要开蓝牙，直接插线，除非你乐意折腾 使用 setxkbmap 而不是 xmodmap 来做 key remapping 不要用 Xorg The Irrational Part 上午十点零一分，端坐到显示器前，深呼吸两次。
Ok，回滚内核版本之后 Screen Lock 卡死的问题果然消失了，我们继续。
进入系统，先看一下昨晚的日志吧：
... 需求目标： 1. 交换 L-Ctrl &amp;amp; Caps-Lock 2. 交换 Win &amp;amp; Alt 当前方案： 使用 xmodmap 自定义配置文件，在 startx 时执行 已知问题： 1. 系统挂起或蓝牙睡眠导致的 keyboard reconnection 会 reset 掉 Win&amp;amp;Alt 的交换 2. 之后重新执行 xmodmap 会导致 L-Ctrl&amp;amp;Caps-Lock 交换回原状，必须执行两次 后续跟进： 1. TTY console switch 快捷键不可用 2. Terminate X 快捷键 CAB(Ctrl-Alt-Backspace) 不可用 3.</description></item><item><title>Use emoji in MySQL</title><link>https://iamgodot.com/posts/use-emoji-in-mysql/</link><pubDate>Sat, 21 Nov 2020 17:22:18 +0800</pubDate><guid>https://iamgodot.com/posts/use-emoji-in-mysql/</guid><description>最近碰到一个服务器报错，排查后发现是参数中包含了 emoji，导致数据库插入记录失败了。
虽然业务上不要求支持，但好奇之下，我还是基于 MySQL 做了个实验。
What&amp;rsquo;s emoji 🧐 关于 emoji 比较官方的解释：
Emoji are pictographs (pictorial symbols) that are typically presented in a colorful form and used inline in text. They represent things such as faces, weather, vehicles and buildings, food and drink, animals and plants, or icons that represent emotions, feelings, or activities.
那么 emoji 是怎么来的呢：
Emoji are &amp;ldquo;picture characters&amp;rdquo; originally associated with cellular telephone usage in Japan, but now popular worldwide.</description></item><item><title>The Art of Readable Code</title><link>https://iamgodot.com/posts/the-art-of-readable-code/</link><pubDate>Sat, 31 Oct 2020 15:54:05 +0800</pubDate><guid>https://iamgodot.com/posts/the-art-of-readable-code/</guid><description>最近读了《编写可读代码的艺术》这本书，收获良多。
整本书的核心都在于一个原则：代码应当易于理解。作者在开篇就提出了可读性的概念：
代码的写法应当使别人理解它所需要的时间最小化。
上述的别人更有可能是未来的自己，所以保证可读性非常有助于节省自己的时间。
Naming There are only two hard things in Computer Science: cache invalidation and naming things. &amp;ndash; Phil Karlton
命名非常重要。不论是变量常量，还是方法对象，一旦确定名称，代码的整体风格就开始受到影响，并且会一直持续下去。
书里介绍的各种技巧，大致都基于信息量和准确性两点。前者可以保证名称足够有意义，同时也检验了其存在的必要性；而后者能够减少代码中的重复定义，还有助于加速 debug 的推导过程。
另外，单词量有时也会影响命名能力。比如 make，作为动词来描述一个操作可能并不够清晰，更好的选择还有 create/generate/setup/compose 等。如果不认识这些单词，就想不到更多更合适的名称。
Comment 给代码加注释是一件很有争议的事情，因此作者也提到：要明确什么时候需要，什么时候不需要。好的代码如同好的文章，自成一体，但这不代表注释就是无意义的。
广义上讲，注释也是另一种形式的文档。单行注释、方法的注解和模块的说明，对于不想了解实现细节的人来说，比代码本身更有价值。
在工作中，写文档的时间并不比编码少。两者并不冲突，因为归根到底都是要把一件事情描述清楚，一个给人看，另一个给机器。文档写得清晰，写代码也会轻松。从使用者的角度看，对于开源项目，我们对文档的关注度更高，在使用中大部分时间都是在查阅手册，而非源代码。
Less is more 在 Python 中写出好看的 Oneliner 很容易，但后果可能是灾难性的，会给代码的修改和调试过程带来意想不到的困难。Debug 时往往需要快速定位问题，如果遇到过于压缩的语句，便很难在短时间内拆解逻辑，更不用说再做修改，此时的代码就像个花瓶，精致而易碎。
Less is more。不考虑可读性的话，代码越少，带来的麻烦反而会越多。
Writing 书里还说把想法变成代码，关键在于是否能把程序要做的事情用自然语言解释清楚。
这和写文章何其相似：命题，描述中心思想，行文通顺，言之有物。如果是写一个函数，那就变成：抽象接口，梳理逻辑，解耦并拆分子任务。
随着时间发展，还要给内容做适当的减法。比如抽取重复的代码逻辑，OOP，用第三方库代替现有功能。对于文章来说则是，删除废话，同样的描述语只保留一个，使用更精简的词汇表达等等。
把编码当成写作，是这件事最吸引我的地方。前者不只是枯燥的堆砌，后者也并非涂鸦般简单，在 Art of readability 这一点上，它们是相通的。</description></item></channel></rss>